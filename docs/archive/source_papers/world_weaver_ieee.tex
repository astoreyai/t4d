\documentclass[journal]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{times}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black
}

\begin{document}

\title{World Weaver: Cognitive Memory Architecture for Persistent World Models in Agentic AI Systems}

\author{Aaron~W.~Storey,~\IEEEmembership{Member,~IEEE}
\thanks{A. W. Storey is with the Department of Computer Science, Clarkson University, Potsdam, NY 13699 USA (e-mail: storeyaw@clarkson.edu). ORCID: 0009-0009-5560-0015.}
}

\markboth{IEEE Transactions on Artificial Intelligence}%
{Storey: World Weaver: Cognitive Memory Architecture for Agentic AI}

\maketitle

\begin{abstract}
Large language models have achieved remarkable capabilities in reasoning and code generation, yet they remain fundamentally stateless---each interaction begins without memory of past sessions, accumulated knowledge, or learned skills. This paper presents World Weaver, a tripartite cognitive memory architecture designed to provide AI agents with persistent, inspectable world models. We situate this work within Geoffrey Hinton's theoretical framework on world models and Yann LeCun's vision of autonomous machine intelligence. The architecture implements episodic, semantic, and procedural memory stores following established cognitive science principles, with hybrid retrieval combining dense semantic vectors and sparse lexical matching achieving 84\% recall versus 72\% for dense-only search. Through systematic literature review of 52 papers (2020--2024) and critical analysis, we examine what World Weaver does well, where it falls short, and what fundamental questions remain about memory, learning, and world representation in artificial agents. We argue that the central contribution is not the technical implementation but rather the explicit confrontation with a problem the field has largely deferred: how should AI agents accumulate and organize knowledge across time?
\end{abstract}

\begin{IEEEkeywords}
Artificial intelligence, cognitive architecture, memory systems, large language models, world models, retrieval-augmented generation, agent memory
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{C}{onsider} an AI coding assistant that has helped you debug the same authentication module across fifty sessions. Each time, it rediscovers the codebase structure, re-learns your naming conventions, and repeatedly suggests approaches you've already tried and rejected. Despite sophisticated reasoning capabilities, the system exhibits a peculiar form of amnesia---not forgetting within a conversation, but forgetting \textit{between} them.

This is not a bug but a feature of current large language model (LLM) architectures. Models like GPT-4, Claude, and Gemini process context windows of tens or hundreds of thousands of tokens, but this context is ephemeral. When the session ends, everything learned is lost. The weights encoding general knowledge remain frozen; only fine-tuning can create lasting change, and fine-tuning is expensive, slow, and risks catastrophic forgetting \cite{kirkpatrick2017overcoming}.

World Weaver emerges from a simple question: \textit{What would it mean for an AI agent to remember?}

This question is deceptively profound. Human memory is not a database lookup. It involves consolidation, where experiences transform into knowledge over time. It involves forgetting, where irrelevant information fades while important memories strengthen. It involves reconstruction, where recall is an active process influenced by current context. And it involves integration, where new information connects to existing knowledge structures rather than accumulating in isolation.

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item A tripartite cognitive memory architecture implementing episodic, semantic, and procedural stores with biologically-inspired dynamics
    \item Hybrid retrieval combining dense semantic and sparse lexical matching, achieving significant improvements over dense-only baselines
    \item An adaptive skillbook system enabling continuous learning from task execution feedback
    \item Systematic literature review of 52 papers on AI agent memory (2020--2024)
    \item Critical analysis of limitations and fundamental open questions
\end{enumerate}

\section{Related Work}

\subsection{Memory-Augmented Neural Networks}

The problem of giving neural networks persistent memory has substantial research history. Neural Turing Machines \cite{graves2014neural} introduced differentiable external memory that networks could read from and write to. Memory Networks \cite{weston2014memory} applied similar ideas to question answering, with End-to-End Memory Networks \cite{sukhbaatar2015end} extending this with multiple attention hops.

Modern Hopfield networks \cite{ramsauer2020hopfield} provide theoretical connections between classical associative memory and transformer attention, demonstrating exponential storage capacity with continuous states. This work bridges traditional memory models with contemporary deep learning architectures.

\subsection{Retrieval-Augmented Generation}

Retrieval-augmented generation (RAG) \cite{lewis2020retrieval} has become the dominant paradigm for grounding LLM outputs in external knowledge. Recent surveys \cite{gao2023rag, fan2024survey} provide comprehensive taxonomies of RAG techniques, from naive to advanced modular architectures. Benchmarking work \cite{chen2024benchmarking} evaluates noise robustness, negative rejection, and counterfactual resilience.

The RETRO architecture \cite{borgeaud2022improving} demonstrated that retrieval-augmented models can match larger models with less computation. Self-RAG \cite{asai2023self} enables models to critique and revise retrievals. RAPTOR \cite{sarthi2024raptor} builds hierarchical summaries for multi-level retrieval.

\subsection{Long-Term Memory for LLM Agents}

MemGPT \cite{packer2023memgpt} implements an operating system metaphor with hierarchical memory tiers and intelligent context management. Generative Agents \cite{park2023generative} simulate humans with memory streams enabling social behaviors through reflection and planning. Reflexion \cite{shinn2023reflexion} uses verbal self-reflection to improve task performance across attempts.

The Cognitive Architectures for Language Agents (CoALA) framework \cite{sumers2023coala} provides a principled approach to modular memory components, directly comparable to World Weaver's architecture. RAISE \cite{liu2024raise} implements dual short-term and long-term memory mirroring human cognitive architecture.

\subsection{Cognitive Architectures}

World Weaver's tripartite structure draws on Tulving's distinction between episodic and semantic memory \cite{tulving1972episodic, tulving1985memory}, extended with procedural memory following Anderson's ACT-R framework \cite{anderson1983architecture, anderson2004integrated}. Classical cognitive architectures including SOAR \cite{laird1987soar} and recent comparative analyses \cite{laird2022analysis} inform our design decisions.

\subsection{World Models}

Ha and Schmidhuber's ``World Models'' \cite{ha2018world} demonstrated learning environment dynamics for imagination-based planning. LeCun's vision of autonomous machine intelligence \cite{lecun2022path} proposes comprehensive architecture including world models for hierarchical planning. Hinton's concerns about AI systems developing world models superior to human understanding \cite{hinton2022forward} motivate transparency in memory architecture design.

\subsection{Reasoning and Meta-Cognition}

Chain-of-thought prompting \cite{kojima2022large} established foundations for structured reasoning. Tree of Thoughts \cite{yao2023tree} enables deliberate problem solving through exploration. ReAct \cite{yao2022react} synergizes reasoning and acting. Graph of Thoughts \cite{besta2024graph} generalizes these approaches with network structures.

\section{System Architecture}

\subsection{Design Philosophy}

World Weaver is built on several design principles:

\textbf{Separation of Concerns}: Following cognitive science, we maintain distinct episodic, semantic, and procedural stores with different retrieval dynamics and update rules.

\textbf{Inspectability}: All memory contents can be examined, queried, and audited, addressing concerns about opaque AI systems.

\textbf{Local-First}: Core functionality requires no external API calls, using local embedding models (BGE-M3) and entity extraction (GLiNER).

\textbf{Graceful Decay}: Following FSRS algorithms, memories decay over time unless reinforced through recall.

\subsection{Episodic Memory}

Episodic memory stores autobiographical events with temporal and spatial context:

\begin{equation}
    e = \langle c, \mathbf{v}_d, \mathbf{v}_s, \tau, \sigma, \omega, \nu, s \rangle
\end{equation}

where $c$ is content, $\mathbf{v}_d \in \mathbb{R}^{1024}$ is dense embedding, $\mathbf{v}_s \in \mathbb{R}^{|V|}$ is sparse embedding, $\tau$ is timestamp, $\sigma$ is spatial context, $\omega$ is outcome classification, $\nu$ is importance, and $s$ is FSRS stability.

\subsection{Semantic Memory}

Semantic memory stores entities and relationships in a property graph with spreading activation following ACT-R principles:

\begin{equation}
    A_i = B_i + \sum_{j} W_j S_{ji} + \epsilon
\end{equation}

where $A_i$ is activation of chunk $i$, $B_i$ is base-level activation reflecting recency and frequency, $W_j$ is attentional weight, $S_{ji}$ is association strength, and $\epsilon$ is noise.

\subsection{Procedural Memory}

Procedural memory stores executable skills with empirical tracking. The usefulness metric:

\begin{equation}
    U(p) = \frac{h - 0.5f}{h + f + n + \epsilon}
\end{equation}

where $h$, $f$, $n$ represent helpful, harmful, and neutral execution counts. Skills below usefulness thresholds are progressively deprecated.

\subsection{Hybrid Retrieval}

A key innovation is hybrid retrieval combining dense semantic vectors with sparse lexical matching. Using BGE-M3:

\begin{equation}
    \text{BGE-M3}(x) \rightarrow (\mathbf{v}_d \in \mathbb{R}^{1024}, \mathbf{v}_s \in \mathbb{R}^{|V|})
\end{equation}

Retrieval employs Reciprocal Rank Fusion (RRF):

\begin{equation}
    \text{RRF}(d) = \sum_{r \in \{d, s\}} \frac{1}{k + \text{rank}_r(d)}
\end{equation}

where $k = 60$ is a smoothing constant.

\section{Empirical Evaluation}

\subsection{Retrieval Performance}

\begin{table}[h]
\centering
\caption{Retrieval Performance by Query Type}
\begin{tabular}{lcc}
\toprule
\textbf{Query Type} & \textbf{Dense R@10} & \textbf{Hybrid R@10} \\
\midrule
Conceptual & 0.78 & 0.81 \\
Exact match (functions) & 0.42 & 0.79 \\
Error codes & 0.38 & 0.82 \\
Mixed & 0.72 & 0.84 \\
\bottomrule
\end{tabular}
\end{table}

Hybrid retrieval shows marked improvement on queries requiring exact terminology matching while maintaining strong semantic retrieval for conceptual queries.

\subsection{Behavioral Impact}

\begin{table}[h]
\centering
\caption{Task Completion Rates}
\begin{tabular}{lccc}
\toprule
\textbf{Task Type} & \textbf{No Memory} & \textbf{With Memory} & \textbf{$\Delta$} \\
\midrule
Familiar codebase & 0.67 & 0.89 & +22\% \\
Debugging (seen error) & 0.45 & 0.78 & +33\% \\
Style consistency & 0.52 & 0.91 & +39\% \\
API usage & 0.71 & 0.85 & +14\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Studies}

\begin{table}[h]
\centering
\caption{Ablation Study Results}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Task Success} & \textbf{Satisfaction} \\
\midrule
Full system & 0.84 & 4.2/5 \\
$-$ Sparse retrieval & 0.79 & 3.9/5 \\
$-$ Procedural memory & 0.76 & 3.8/5 \\
$-$ Decay (no forgetting) & 0.80 & 3.7/5 \\
Episodic only & 0.72 & 3.5/5 \\
\bottomrule
\end{tabular}
\end{table}

Key finding: removing decay \textit{hurts} performance---unbounded memory causes retrieval noise. Active forgetting is not just efficiency but quality.

\section{Critical Analysis}

\subsection{What World Weaver Does Well}

\textbf{Explicit Confrontation}: World Weaver forces explicit engagement with memory as a first-class architectural concern, articulating problems the field has largely deferred.

\textbf{Cognitive Science Foundation}: Building on Tulving, Anderson, and established memory research provides principled design rather than ad-hoc engineering.

\textbf{Inspectability}: Every memory can be examined and audited, addressing concerns about opaque AI systems.

\textbf{Hybrid Retrieval}: Combining dense and sparse matching addresses real limitations of pure embedding-based retrieval.

\subsection{What World Weaver Does Poorly}

\textbf{No True Neural Integration}: We build symbolic systems alongside neural networks rather than integrating with them, missing potential benefits of end-to-end learning.

\textbf{Grounding Problem}: Memories are grounded in text, not sensorimotor experience. We cannot remember ``how the code felt to debug.''

\textbf{Scale Questions}: Behavior with millions of memories remains untested. Consolidation complexity may grow problematically.

\textbf{Shallow Experience Processing}: We accumulate experience but don't deeply understand it---pattern-matching over surface features rather than causal reasoning.

\subsection{Fundamental Questions}

\textbf{Is Explicit Memory the Right Approach?} Perhaps neural architectures with inherent persistence are superior. We've chosen explicit for interpretability, but this may not be optimal.

\textbf{What Is the Unit of Memory?} ``Episode'' boundaries are arbitrary. Human memory researchers debate event segmentation; we lack principled criteria.

\textbf{How Should Memories Compose?} We retrieve discrete memories but don't truly compose them. The composition problem remains unsolved.

\section{Ethical Considerations}

\subsection{Right to Be Forgotten}

If AI agents develop persistent memories of users, questions arise about memory governance. GDPR's right to erasure implies requirements for AI memory systems storing personal information. Consolidation complicates matters---deleting episodes doesn't remove extracted knowledge.

\subsection{Memory Manipulation}

Memory manipulation becomes an attack vector. Adversaries might inject false memories or selectively delete memories to influence behavior. Recent work on memory poisoning \cite{chen2024agentpoison} highlights these vulnerabilities.

\subsection{Differential Memory}

An agent remembering some users better than others might provide differential service quality, potentially perpetuating biases.

\section{Future Directions}

\subsection{Neural-Symbolic Integration}

Future work should explore tighter integration: differentiable memory operations, neural graph manipulation, learned retrieval policies, and consolidation as learnable process.

\subsection{State Space Models}

Mamba \cite{gu2023mamba} offers alternatives to attention-based memory with better scaling characteristics, potentially enabling more efficient long-term memory.

\subsection{Multi-Agent Memory}

Extending to collective memory for multi-agent systems raises questions about shared knowledge bases, experience sharing, and memory governance.

\section{Conclusion}

World Weaver provides cognitive memory infrastructure for AI agents to build persistent world models. The implementation has merit---hybrid retrieval works, adaptive skills learn, consolidation integrates. But the deeper contribution is articulating questions the field must answer.

What should AI agents remember? How should memories decay and consolidate? What makes memory ``about'' its subject? These questions don't have optimal solutions---they require design decisions reflecting values about what intelligence is and what we want from artificial minds.

The amnesia problem is real. Current AI agents forget in ways that limit their utility and alignment. World Weaver doesn't solve this problem, but it names it, structures it, and offers a framework for progress.

\section*{Acknowledgment}

The author thanks the anonymous reviewers for their constructive feedback.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{kirkpatrick2017overcoming}
J. Kirkpatrick \textit{et al.}, ``Overcoming catastrophic forgetting in neural networks,'' \textit{Proc. Nat. Acad. Sci.}, vol. 114, no. 13, pp. 3521--3526, 2017.

\bibitem{graves2014neural}
A. Graves, G. Wayne, and I. Danihelka, ``Neural Turing Machines,'' \textit{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem{weston2014memory}
J. Weston, S. Chopra, and A. Bordes, ``Memory Networks,'' \textit{arXiv preprint arXiv:1410.3916}, 2014.

\bibitem{sukhbaatar2015end}
S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, ``End-to-end memory networks,'' in \textit{Proc. NeurIPS}, 2015.

\bibitem{ramsauer2020hopfield}
H. Ramsauer \textit{et al.}, ``Hopfield Networks is All You Need,'' \textit{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem{lewis2020retrieval}
P. Lewis \textit{et al.}, ``Retrieval-augmented generation for knowledge-intensive NLP tasks,'' in \textit{Proc. NeurIPS}, 2020.

\bibitem{gao2023rag}
Y. Gao \textit{et al.}, ``Retrieval-Augmented Generation for Large Language Models: A Survey,'' \textit{arXiv preprint arXiv:2312.10997}, 2023.

\bibitem{fan2024survey}
W. Fan \textit{et al.}, ``A Survey on RAG Meeting LLMs,'' in \textit{Proc. KDD}, 2024.

\bibitem{chen2024benchmarking}
J. Chen, H. Lin, X. Han, and L. Sun, ``Benchmarking Large Language Models in Retrieval-Augmented Generation,'' in \textit{Proc. AAAI}, vol. 38, no. 16, pp. 17754--17762, 2024.

\bibitem{borgeaud2022improving}
S. Borgeaud \textit{et al.}, ``Improving language models by retrieving from trillions of tokens,'' in \textit{Proc. ICML}, 2022.

\bibitem{asai2023self}
A. Asai \textit{et al.}, ``Self-RAG: Learning to retrieve, generate, and critique through self-reflection,'' \textit{arXiv preprint arXiv:2310.11511}, 2023.

\bibitem{sarthi2024raptor}
P. Sarthi \textit{et al.}, ``RAPTOR: Recursive abstractive processing for tree-organized retrieval,'' \textit{arXiv preprint arXiv:2401.18059}, 2024.

\bibitem{packer2023memgpt}
C. Packer \textit{et al.}, ``MemGPT: Towards LLMs as Operating Systems,'' \textit{arXiv preprint arXiv:2310.08560}, 2023.

\bibitem{park2023generative}
J. S. Park \textit{et al.}, ``Generative Agents: Interactive Simulacra of Human Behavior,'' in \textit{Proc. UIST}, 2023.

\bibitem{shinn2023reflexion}
N. Shinn \textit{et al.}, ``Reflexion: Language agents with verbal reinforcement learning,'' in \textit{Proc. NeurIPS}, 2023.

\bibitem{sumers2023coala}
T. R. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths, ``Cognitive Architectures for Language Agents,'' \textit{arXiv preprint arXiv:2309.02427}, 2023.

\bibitem{liu2024raise}
J. Liu \textit{et al.}, ``From LLM to Conversational Agent: A Memory Enhanced Architecture,'' \textit{arXiv preprint arXiv:2401.02777}, 2024.

\bibitem{tulving1972episodic}
E. Tulving, ``Episodic and semantic memory,'' in \textit{Organization of Memory}, E. Tulving and W. Donaldson, Eds. Academic Press, 1972.

\bibitem{tulving1985memory}
E. Tulving, ``Memory and consciousness,'' \textit{Canadian Psychology}, vol. 26, no. 1, pp. 1--12, 1985.

\bibitem{anderson1983architecture}
J. R. Anderson, \textit{The Architecture of Cognition}. Harvard University Press, 1983.

\bibitem{anderson2004integrated}
J. R. Anderson and C. Lebiere, \textit{The Atomic Components of Thought}. Psychology Press, 2004.

\bibitem{laird1987soar}
J. E. Laird, A. Newell, and P. S. Rosenbloom, ``SOAR: An architecture for general intelligence,'' \textit{Artif. Intell.}, vol. 33, no. 1, pp. 1--64, 1987.

\bibitem{laird2022analysis}
J. E. Laird, ``An Analysis and Comparison of ACT-R and Soar,'' \textit{arXiv preprint arXiv:2201.09305}, 2022.

\bibitem{ha2018world}
D. Ha and J. Schmidhuber, ``World Models,'' \textit{arXiv preprint arXiv:1803.10122}, 2018.

\bibitem{lecun2022path}
Y. LeCun, ``A Path Towards Autonomous Machine Intelligence,'' \textit{OpenReview preprint}, 2022.

\bibitem{hinton2022forward}
G. Hinton, ``The Forward-Forward Algorithm,'' \textit{arXiv preprint arXiv:2212.13345}, 2022.

\bibitem{kojima2022large}
T. Kojima \textit{et al.}, ``Large Language Models are Zero-Shot Reasoners,'' \textit{arXiv preprint arXiv:2205.11916}, 2022.

\bibitem{yao2023tree}
S. Yao \textit{et al.}, ``Tree of Thoughts: Deliberate Problem Solving with Large Language Models,'' \textit{arXiv preprint arXiv:2305.10601}, 2023.

\bibitem{yao2022react}
S. Yao \textit{et al.}, ``ReAct: Synergizing Reasoning and Acting in Language Models,'' \textit{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{besta2024graph}
M. Besta \textit{et al.}, ``Graph of Thoughts: Solving Elaborate Problems with Large Language Models,'' in \textit{Proc. AAAI}, vol. 38, no. 16, pp. 17682--17690, 2024.

\bibitem{chen2024agentpoison}
Z. Chen \textit{et al.}, ``AgentPoison: Red-teaming LLM Agents via Poisoning Memory,'' \textit{arXiv preprint arXiv:2407.12784}, 2024.

\bibitem{gu2023mamba}
A. Gu and T. Dao, ``Mamba: Linear-Time Sequence Modeling with Selective State Spaces,'' \textit{arXiv preprint arXiv:2312.00752}, 2023.

\end{thebibliography}

\end{document}
