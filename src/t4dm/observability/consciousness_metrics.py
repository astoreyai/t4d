"""
IIT Consciousness Metrics (W3-04).

Compute Integrated Information Theory (IIT) inspired metrics
for system awareness and global workspace monitoring.

Evidence Base: Tononi (2004) "An information integration theory of consciousness"

Key Insight:
    Phi (Φ) measures how much information is generated by a system
    above and beyond its parts. High Φ indicates integrated processing,
    a hallmark of conscious systems.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, Callable, Optional

import torch

if TYPE_CHECKING:
    pass

logger = logging.getLogger(__name__)


@dataclass
class ConsciousnessMetrics:
    """Integrated Information Theory metrics.

    Attributes:
        phi: Integrated information (Φ). Higher = more integrated.
        surprise: |E_current - E_previous|. Energy change.
        integration: Mutual info between subsystems.
        differentiation: Entropy of activation patterns.
        conscious_threshold: Threshold for "conscious" processing.
    """

    phi: float
    surprise: float
    integration: float
    differentiation: float
    conscious_threshold: float = 0.5

    @property
    def is_conscious(self) -> bool:
        """Whether phi exceeds consciousness threshold."""
        return self.phi > self.conscious_threshold


class IITMetricsComputer:
    """Compute IIT-inspired consciousness metrics.

    Φ (phi) measures how much information is generated by a system
    above and beyond its parts. We use approximations since exact
    computation is intractable.

    Components:
    - Integration: Mutual information between subsystems
    - Differentiation: Entropy of activation patterns
    - Surprise: Energy change from previous state
    - Phi: Integration * Differentiation (simplified)

    Example:
        >>> def energy_fn(x): return torch.sum(x ** 2).item()
        >>> computer = IITMetricsComputer(energy_fn)
        >>> metrics = computer.compute(spiking_output, memory_state)
        >>> if metrics.is_conscious:
        ...     print("System is integrated")
    """

    def __init__(self, energy_fn: Callable[[torch.Tensor], float]):
        """Initialize IIT metrics computer.

        Args:
            energy_fn: Function to compute energy from activation tensor.
        """
        self.energy_fn = energy_fn
        self.previous_energy: Optional[float] = None

    def compute(
        self,
        spiking_output: torch.Tensor,
        memory_state: torch.Tensor,
    ) -> ConsciousnessMetrics:
        """Compute consciousness metrics from current state.

        Φ = Σ I(A; B) for all bipartitions - not exactly computable,
        so we use approximations.

        Args:
            spiking_output: Output from spiking network [batch, features].
            memory_state: Current memory state [batch, features].

        Returns:
            ConsciousnessMetrics with phi, surprise, integration, differentiation.
        """
        # Surprise = energy change
        current_energy = self.energy_fn(spiking_output)
        if self.previous_energy is not None:
            surprise = abs(current_energy - self.previous_energy)
        else:
            surprise = 0.0
        self.previous_energy = current_energy

        # Integration = mutual info between spiking and memory
        integration = self._mutual_information(spiking_output, memory_state)

        # Differentiation = entropy of activation pattern
        differentiation = self._entropy(spiking_output)

        # Φ approximation (simplified): product of integration and differentiation
        # True IIT uses minimum information partition, which is intractable
        phi = integration * differentiation

        return ConsciousnessMetrics(
            phi=phi,
            surprise=surprise,
            integration=integration,
            differentiation=differentiation,
        )

    def _mutual_information(self, x: torch.Tensor, y: torch.Tensor) -> float:
        """Estimate mutual information via correlation.

        Uses Pearson correlation as a proxy for mutual information.
        True MI requires density estimation which is expensive.

        Args:
            x: First tensor.
            y: Second tensor.

        Returns:
            Absolute correlation in [0, 1].
        """
        x_flat = x.flatten()
        y_flat = y.flatten()

        # Match lengths
        min_len = min(len(x_flat), len(y_flat))
        x_flat = x_flat[:min_len]
        y_flat = y_flat[:min_len]

        # Handle edge case of constant values
        if x_flat.std() < 1e-8 or y_flat.std() < 1e-8:
            return 0.0

        # Compute correlation
        corr = torch.corrcoef(torch.stack([x_flat, y_flat]))[0, 1]

        # Handle NaN
        if torch.isnan(corr):
            return 0.0

        return abs(corr.item())

    def _entropy(self, x: torch.Tensor) -> float:
        """Estimate entropy of activation pattern.

        Discretizes values into bins and computes Shannon entropy.

        Args:
            x: Activation tensor.

        Returns:
            Entropy in nats (natural log base).
        """
        x_flat = x.flatten()

        # Handle constant tensor
        if x_flat.std() < 1e-8:
            return 0.0

        # Discretize to bins
        hist = torch.histc(x_flat, bins=100)
        probs = hist / hist.sum()
        probs = probs[probs > 0]

        # Shannon entropy
        entropy = -torch.sum(probs * torch.log(probs + 1e-10))

        return entropy.item()

    def reset(self) -> None:
        """Reset previous energy state."""
        self.previous_energy = None
